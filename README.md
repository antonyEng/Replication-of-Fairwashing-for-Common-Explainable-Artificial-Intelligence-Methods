Replication of Fairwashing for Common Explainable Artificial Intelligence Methods: In this project, 
the algorithm developed by (Dimanov et al., 2020) -You Shouldn't Trust Me: Learning Models Which Conceal Unfairness From Multiple Explanation Methods- 
to create a biased ML model has been used and applied to the logistic regression algorithm.

LIME showed that it could not reveal necesarry imformation about the ML model and the data used, in the example investigated, it was not able to tell either a biased ML has been used or 
a special feature has low influence. A counterfactual method has showed that the coefficient given to this feature is very small and that the reason behind the results. Of course several 
tchniques could be used as counterfactual as using another ML algorithm and comparing the results using LIME. 
